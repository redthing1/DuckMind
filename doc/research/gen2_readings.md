# duckmind gen2 readings

## LGOAP: Adaptive layered planning for real-time videogames
https://www.researchgate.net/publication/261452466_LGOAP_Adaptive_layered_planning_for_real-time_videogames

[LGOAP adaptive layered planning for real-time videogames.pdf](../papers/LGOAP.adaptive.layered.planning.for.real-time.videogames.pdf)


> To reduce the complexity of the algorithm we have adopted a hierarchical system similar to [13]. This hierarchical system uses multiple planners nested inside one another. The highest level planners plan actions that span a long time each and which involve large changes in resources. As lower level planners are activated, their plans are created so that they respect the constraints given by the higher layers. The actions of a layer are more concrete, take less time, and generally involve smaller resource exchanges than the actions of the layers in higher levels. Considering three layers, examples of actions for the example seen in Section III could be: (i) complete quest X, acquire sword, fight ogre for the highest layer; (ii) use item X for the middle level layer; and move to X for the lowest level layer. Of course, depending on the problem domain, different layers and actions may need to be deﬁned.

> The layered planning system creates plans by activating a lower level layer to specify the actions that must be taken in order to complete a higher level action. The ﬁnal planning system works by invoking the fast planning function multiple times, each time with a different set of available actions (those identiﬁed by the current layer) and with a different goal (the one speciﬁed by the current higher layer action).

> A layered approach offers a further reduction in computational load, which applies to non-deterministic simulations where planning may fail due to unforeseen circumstances. Instead of computing all the steps needed by all the layers, we can employ a lazy evaluation [14] approach where plans are reﬁned only as the previous actions get completed. This way, if a plan fails halfway, we can skip computing the lower level plans for the successive actions which are discarded anyway.

> In an effort to improve the performance and believability of our NPCs, in our approach we provided them with a small memory of old plans, so that a set of useful plans that were found and used in the past may be stored for each layer. This process is known as memoization or tabling [15], and has been used in conjunction with planners such as [16]. Each plan contains not only a sequence of actions, but also the circumstances it was activated in (time, location, NPC resources). If the current circumstances match the original circumstances that the plan was activated in, then the plan is considered a candidate for re-activation. The beneﬁts for the plan are then computed from the current conﬁguration of the game world, and if the result achieves the current goal then the plan is used. When a plan is used, then its score is increased according to some criterion, for example the amount of time elapsed since last reuse. When a new plan is formulated, then it is memorized, but if there are too many plans already in memory then the lowest scoring plans are removed since they have been used the least.

> When planning in real-time, special care must be taken in order to ensure that the executed plans remain sensible throughout their execution. This cannot be guaranteed while computing a plan, since we cannot predict with total accuracy how lower level plans will achieve goals, how other agents will act, and how random events in the game world will affect the state. This requires plan executions to be interrupted in order to plan again whenever appropriate. Moreover, there can be beneﬁt in searching for new plans while waiting for the execution of the current plan to be completed. In particular, this minimizes the time an NPC is idle while waiting for its planner to complete.

> Traditional planning systems: LGOAP borrows and heavily extends the central concepts of previous work such as STRIPS and GOAP [3], [7]. The main difference between our approach and those is the use of layering to provide a scalable mechanism to cope with the explosion in the search tree that occurs when searching for long sequences of actions. By using layers we effectively tame the complexity of the algorithm, without giving up the original power of the techniques.

> An additional aspect that could be explored is that of social interactions through cooperative planning between NPCs. By planning to meet or cooperate at given times in the higher level planners, it would be possible to deﬁne simple social behaviours such as dates, or even more complex ones such as team combat. Social interaction may allow agents to communicate and even share plans and other useful information. NPCs would then learn faster through such interactions.

## Utility Models for Goal‐Directed, Decision‐Theoretic Planners
https://onlinelibrary.wiley.com/doi/abs/10.1111/0824-7935.00068

[Utility Models for Goal‐Directed, Decision‐Theoretic Planners.pdf](../papers/Utility.Models.for.Goal.Directed.Decision.Theoretic.Planners.pdf)

> In this paper we seek a middle ground between the simple goal model and the arbitrary preference model. We extend the notion of a goal into a preference or utility model that includes concepts like temporal extent, partial satisfaction, and efﬁciency in achievement. At the same time, we retain the explicit structural information in the goal formula. Thus, planning algorithms can still use the preference model to guide the search for optimal plans by evaluating partially constructed ones.

> We should also note that the assumption of utility independence does not imply that goals are probabilistically independent. One might object that two goals—“have the truck at the depot by noon” and “have the truck clean”—interact strongly if the only road to the depot is muddy, and there is no way to wash the truck once it arrives at the depot. In particular, there might be no plan that would make them both true. But this situation means only that the two goals are not probabilistically independent; it does not constitute a violation of utility independence. The MUI assumption demands only that the utility derived from satisfying one top-level goal does not depend on the extent to which the other goals are achieved. It does not address the likelihood of achieving either goal in isolation, or both simultaneously. The likelihood of achieving goals is properly reﬂected in the probabilistic model of the domain and the operators. It is not a reﬂection of the decision maker’s fundamental preferences.

> In classical planning algorithms, goals consist of a symbolic expression to be achieved. Our paper extends this idea in several directions, including giving goals a temporal extent and allowing them to be partially satisﬁable. Partial satisfaction should be allowed for both numeric goals (“deliver 10 tons of cargo”) and for more traditional symbolic goals (“have all the parts painted and on the loading dock by noon”). Partial satisfaction should extend to the temporal component as well: if the deadline is noon, ﬁnishing by 12:05 might be better than ﬁnishing the next morning.

> One of the main goals of this paper is to use information in the utility function’s structure to guide the building of good plans, which generally involves demonstrating that one plan is preferable to another. At worst, establishing this relationship involves computing the expected utility of both plans, a process that requires generating and evaluating all possible outcomes for each. The two main problems with this approach are that: (1) generating and evaluating all possible outcomes of a plan can be prohibitively expensive (Hanks 1990a) and (2) a plan cannot be evaluated, and thus alternative plans cannot be compared, until it is complete (i.e., fully generated or reﬁned). We need ways to compare plans which are cheaper than computing expected utilities over all possible outcomes and which can be applied to plans as they are being constructed.

> The notion of partially satisﬁed goals and their role in the decision-making process appears prominently in the literature on fuzzy mathematics and decision analysis. In particular, our notion of a degree of satisfaction function closely resembles a fuzzy-set membership function. The seminal paper in this area is Bellman and Zadeh (1980); also see the papers in Zimmerman, Zadeh, and Gaines (1984), of which the most relevant to this paper is Dubois and Prade (1984), who discuss the role of aggregation operators in the decision-making process. In the language of fuzzy-set theory, a goal may be expressed as a fuzzy set, and a plan’s membership function with respect to that set indicates the extent to which the plan satisﬁes that goal. An aggregation operator combines membership functions for individual goals into an aggregate membership function, which is an indicator of global success; this is called the decision set. A decision maker then selects an alternative that is “strongly” a member of the decision set. Dubois and Prade categorize and analyze various aggregation functions. However, they do not address the computational issues associated with plan evaluation or generation.

## Drive-Based Utility-Maximizing Computer Game Non-Player Characters
https://arrow.tudublin.ie/sciendoc/176/

[Drive-Based Utility-Maximizing Computer Game Non-Player Character.pdf](../papers/Drive-Based.Utility-Maximizing.Computer.Game.Non-Player.Character.pdf)


describes in detail the UDGOAP architecture 

![77-Figure3 3-1](https://user-images.githubusercontent.com/54390138/118928871-aab01a00-b8f8-11eb-9bd3-88c8cb11b918.png)
![78-Figure3 4-1](https://user-images.githubusercontent.com/54390138/118928874-ab48b080-b8f8-11eb-80cd-b8bdb20b93f7.png)
![96-Figure3 5-1](https://user-images.githubusercontent.com/54390138/118928877-ab48b080-b8f8-11eb-951c-fa8bd49296d3.png)
![111-Figure3 8-1](https://user-images.githubusercontent.com/54390138/118928882-abe14700-b8f8-11eb-84e7-b73af81b6eba.png)

## Feeling the ambiance: using smart ambiance to increase contextual awareness in game agents
https://dl.acm.org/doi/10.1145/2159365.2159418

[Smart Ambiance to Increase Contextual Awareness in Game Agents.pdf](../papers/Smart.Ambiance.to.Increase.Contextual.Awareness.in.Game.Agents.pdf)

> This paper introduces smart ambiance as an attempt to address the problem of contextually unrealistic behaviour by having the objects in the environment of an agent implicitly aﬀect the actions selected by the agent. We will present the mechanics of smart ambiance and through examples illustrate how it can create more interesting and believable game agent behaviours.

> Our approach extends GOAP to include smart ambiance, a representation of the general mood or atmosphere within an environment in which an agent is planning actions. The ambiance can arise from the type of an environment, the types of objects in an environment, and recent events that have taken place in an environment, and is deﬁned through a collection of property-value pairs, or ambiance e ﬀ ects. The property is a descriptor of the ambiance eﬀect. An ambiance eﬀect indicates the type of ambiance present (e.g. subdued, seriousness, happy, sad, fearful, exciting 1 ) and the strength of this ambiance. Each agent has a unique smart ambiance based on the environment it is in, nearby objects, and the events the agent has witnessed.

> Actions in a GOAP system normally have an associated cost and a list of conditions and eﬀects. We have extended GOAP actions to also contain a list of action ambiance details which are ambiance eﬀects that a planning agent should take into account when considering the action for inclusion in a plan. If an ambiance eﬀect is present in the smart ambiance in which an agent is forming a plan, and the same ambiance eﬀect is deﬁned to impact an action through the inclusion of an action ambiance detail, then the likelihood of including the action in a plan will be altered. This change is achieved by altering the cost associated with an action according to the action ambiance detail (which indicates whether the likelihood of inclusion of an action in a plan should increase or decrease in the presence of an ambience eﬀect) and the strength of the ambiance eﬀect.

> Using the proposed framework, agent action selection is altered by a smart ambiance modifying the costs of particular actions. There are several advantages to using smart ambiance: (1) emergent behaviour is quite likely to occur through the interactions of several objects altering ambiance properties, consequently less scripting on agent behaviour will be needed; (2) agents will perform more contextually sensitive actions; (3) information about how an agent should behave is decentralised away from the agent and across the environment; (4) and many popular game engines already have the data structures (such as triggers) required to implement smart ambiance. But smart ambiance also has several disadvantages: (1) a new cost function must be made for each ambiance property, (2) smart ambiance can at times be computationally a little bulkier than what is strictly necessary to create the eﬀects that it leads to; (3) and emergent behaviour is often seen as bad by game designers as there is a loss of control from the perspective of the designer. The system is also more computationally demanding than ordinary GOAP, which is already considerably more demanding than other agent behaviour control systems.
